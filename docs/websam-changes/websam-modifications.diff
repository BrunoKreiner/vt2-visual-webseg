diff --git a/linter.sh b/linter.sh
old mode 100755
new mode 100644
diff --git a/segment_anything/build_sam.py b/segment_anything/build_sam.py
index 37cd245..c1bd5e8 100644
--- a/segment_anything/build_sam.py
+++ b/segment_anything/build_sam.py
@@ -11,36 +11,42 @@ from functools import partial
 from .modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, Sam, TwoWayTransformer
 
 
-def build_sam_vit_h(checkpoint=None):
+def build_sam_vit_h(checkpoint=None, strict_weights = True, freeze_encoder= True):
     return _build_sam(
         encoder_embed_dim=1280,
         encoder_depth=32,
         encoder_num_heads=16,
         encoder_global_attn_indexes=[7, 15, 23, 31],
         checkpoint=checkpoint,
+        strict_weights = strict_weights,
+        freeze_encoder= freeze_encoder
     )
 
 
 build_sam = build_sam_vit_h
 
 
-def build_sam_vit_l(checkpoint=None):
+def build_sam_vit_l(checkpoint=None, strict_weights = True, freeze_encoder= True):
     return _build_sam(
         encoder_embed_dim=1024,
         encoder_depth=24,
         encoder_num_heads=16,
         encoder_global_attn_indexes=[5, 11, 17, 23],
         checkpoint=checkpoint,
+        strict_weights = strict_weights,
+        freeze_encoder= freeze_encoder
     )
 
 
-def build_sam_vit_b(checkpoint=None):
+def build_sam_vit_b(checkpoint=None, strict_weights = True, freeze_encoder= True):
     return _build_sam(
         encoder_embed_dim=768,
         encoder_depth=12,
         encoder_num_heads=12,
         encoder_global_attn_indexes=[2, 5, 8, 11],
         checkpoint=checkpoint,
+        strict_weights = strict_weights,
+        freeze_encoder= freeze_encoder
     )
 
 
@@ -57,7 +63,9 @@ def _build_sam(
     encoder_depth,
     encoder_num_heads,
     encoder_global_attn_indexes,
-    checkpoint=None,
+    checkpoint=None, 
+    strict_weights = True,
+    freeze_encoder= True,
 ):
     prompt_embed_dim = 256
     image_size = 1024
@@ -99,9 +107,24 @@ def _build_sam(
         pixel_mean=[123.675, 116.28, 103.53],
         pixel_std=[58.395, 57.12, 57.375],
     )
-    sam.eval()
+    #sam.eval()
+
+    print(checkpoint)
     if checkpoint is not None:
-        with open(checkpoint, "rb") as f:
-            state_dict = torch.load(f)
-        sam.load_state_dict(state_dict)
+        """with open(checkpoint, "rb") as f:
+            state_dict = torch.load(f)"""
+        sam.load_state_dict(torch.load(checkpoint), strict=strict_weights)
+    
+    # WEBSAM training
+    # First make all parameters require gradients
+    for param in sam.parameters():
+        param.requires_grad = True
+
+    # Then selectively freeze specific parts
+    if freeze_encoder:
+        for name, param in sam.image_encoder.named_parameters():
+            # Don't freeze the adapter components
+            if 'patch_embedding_tune' not in name and 'edge_component_tune' not in name and 'adapters' not in name:
+                param.requires_grad = False
+
     return sam
diff --git a/segment_anything/modeling/image_encoder.py b/segment_anything/modeling/image_encoder.py
index 66351d9..ddf4fcb 100644
--- a/segment_anything/modeling/image_encoder.py
+++ b/segment_anything/modeling/image_encoder.py
@@ -33,6 +33,7 @@ class ImageEncoderViT(nn.Module):
         rel_pos_zero_init: bool = True,
         window_size: int = 0,
         global_attn_indexes: Tuple[int, ...] = (),
+        adapter_dim: Optional[int] = None,
     ) -> None:
         """
         Args:
@@ -62,6 +63,17 @@ class ImageEncoderViT(nn.Module):
             embed_dim=embed_dim,
         )
 
+        #WEB-SAM: Add PatchEmbeddingTune and EdgeComponentsTune
+        self.patch_embedding_tune = PatchEmbeddingTune(embed_dim = embed_dim, scale_factor=8)
+        self.edge_component_tune = EdgeComponentsTune(
+            kernel_size=(16, 16),
+            stride=(16, 16),
+            padding=(0, 0),
+            in_chans=1,
+            embed_dim=embed_dim,
+            scale_factor=8  # As recommended by the paper, mu=8 → output dim becomes 768/8 = 96.
+        )
+
         self.pos_embed: Optional[nn.Parameter] = None
         if use_abs_pos:
             # Initialize absolute positional embedding with pretrain image size.
@@ -85,6 +97,17 @@ class ImageEncoderViT(nn.Module):
             )
             self.blocks.append(block)
 
+        if adapter_dim is None:
+            adapter_dim = embed_dim // 2
+
+        self.adapters = nn.ModuleList([
+            Adapter(
+                embed_dim=embed_dim,
+                adapter_dim=adapter_dim,  # From paper's implementation details
+                scale_factor=8
+            ) for _ in range(depth)
+        ])
+
         self.neck = nn.Sequential(
             nn.Conv2d(
                 embed_dim,
@@ -104,15 +127,28 @@ class ImageEncoderViT(nn.Module):
         )
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
+        grey_scale_x = x.mean(dim=1, keepdim=True)
+        #print("Input shape:", x.shape)
         x = self.patch_embed(x)
+        #print("After patch_embed:", x.shape)
+        F_pet = self.patch_embedding_tune(x)
+        #print("F_pet shape:", F_pet.shape)
+        F_ect = self.edge_component_tune(grey_scale_x) #grey scale first
+        #print("F_ect shape:", F_ect.shape)
         if self.pos_embed is not None:
             x = x + self.pos_embed
+        #print("x shape after pos_embed: ", x.shape)
+        F_merge = F_ect + F_pet
+        #print("F_merge shape:", F_merge.shape)
 
-        for blk in self.blocks:
-            x = blk(x)
+        for blk, adapter in zip(self.blocks, self.adapters):
+            adapter_out = adapter(F_merge)
+            #print("adapter_out shape:", adapter_out.shape)
+            #print("adapter_out shape: ", adapter_out.shape)
+            x = blk(x + adapter_out)
+            #print("block x shape: ", x.shape)
 
         x = self.neck(x.permute(0, 3, 1, 2))
-
         return x
 
 
@@ -393,3 +429,127 @@ class PatchEmbed(nn.Module):
         # B C H W -> B H W C
         x = x.permute(0, 2, 3, 1)
         return x
+    
+class PatchEmbeddingTune(nn.Module):
+    def __init__(self, embed_dim: int, scale_factor: int):
+        super(PatchEmbeddingTune, self).__init__()
+        self.Lpet = nn.Linear(embed_dim, embed_dim // scale_factor)  # E → E/μ
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        return self.Lpet(x)
+
+class EdgeComponentsTune(nn.Module):
+    """
+    Edge Components Tune module that applies the Sobel operator, partitions the resulting edge map 
+    into non-overlapping patches, and projects the patch embeddings to a reduced e-dimensional space.
+    """
+    def __init__(
+        self,
+        kernel_size: Tuple[int, int] = (16, 16),
+        stride: Tuple[int, int] = (16, 16),
+        padding: Tuple[int, int] = (0, 0),
+        in_chans: int = 1,       # Input is grayscale
+        embed_dim: int = 768,    # The dimension after patch partitioning
+        scale_factor: int = 8,   # µ: projection reduces dimension: e = embed_dim / scale_factor
+    ) -> None:
+        super().__init__()
+        
+        # Set up fixed Sobel kernels (for edge detection)
+        self.sobel_x = nn.Conv2d(
+            in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False
+        )
+        self.sobel_y = nn.Conv2d(
+            in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False
+        )
+        # Define Sobel kernels as fixed weights:
+        sobel_kernel_x = torch.tensor(
+            [[-1, 0, 1],
+             [-2, 0, 2],
+             [-1, 0, 1]], dtype=torch.float32
+        ).unsqueeze(0).unsqueeze(0)  # shape: (1,1,3,3)
+        
+        sobel_kernel_y = torch.tensor(
+            [[-1, -2, -1],
+             [ 0,  0,  0],
+             [ 1,  2,  1]], dtype=torch.float32
+        ).unsqueeze(0).unsqueeze(0)  # shape: (1,1,3,3)
+        
+        # Register these kernels as fixed parameters (they won't be updated during training)
+        self.sobel_x.weight = nn.Parameter(sobel_kernel_x, requires_grad=False)
+        self.sobel_y.weight = nn.Parameter(sobel_kernel_y, requires_grad=False)
+        
+        # Partitioning: use a convolution that splits the edge image into patches.
+        self.patch_embed = nn.Conv2d(
+            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding
+        )
+        
+        # Projection: Reduce the dimension from embed_dim to embed_dim // scale_factor.
+        self.proj = nn.Linear(embed_dim, embed_dim // scale_factor)
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        # Fixed Sobel kernels
+        sobel_kernel_x = torch.tensor(
+            [[-1, 0, 1],
+            [-2, 0, 2],
+            [-1, 0, 1]], dtype=torch.float32, device=x.device
+        ).unsqueeze(0).unsqueeze(0)
+        
+        sobel_kernel_y = torch.tensor(
+            [[-1, -2, -1],
+            [ 0,  0,  0],
+            [ 1,  2,  1]], dtype=torch.float32, device=x.device
+        ).unsqueeze(0).unsqueeze(0)
+        
+        # Apply convolution using F.conv2d
+        grad_x = F.conv2d(x, sobel_kernel_x.expand(1, 1, 3, 3), padding=1)
+        grad_y = F.conv2d(x, sobel_kernel_y.expand(1, 1, 3, 3), padding=1)
+        # Compute the gradient magnitude
+        sobel = torch.sqrt(grad_x ** 2 + grad_y ** 2)
+        #print("sobel shape: ", sobel.shape)
+        # 2. Partition the Sobel edge map into patches (as in the original PatchEmbed)
+        patches = self.patch_embed(sobel)  # [B, embed_dim, H_p, W_p]
+        #print("patches shape: ", patches.shape)
+        # Rearrange to shape [B, H_p, W_p, embed_dim]
+        patches = patches.permute(0, 2, 3, 1)
+        #print("patches shape after permute: ", patches.shape)
+        
+        # 3. Project each patch embedding to a lower-dimensional space:
+        patches_proj = self.proj(patches)  # [B, H_p, W_p, embed_dim // scale_factor]
+        
+        return patches_proj
+    
+class Adapter(nn.Module):
+    """
+    Adapter module for integrating edge features into the image encoder.
+
+    Args:
+        embed_dim (int): Dimension of the input and output features.
+        adapter_dim (int): Dimension of the adapter features.
+
+    Attributes:
+        MLPktune (nn.Linear): Linear layer for tuning the adapter features.
+        MLPup (nn.Linear): Linear layer for projecting the tuned adapter features back to the original dimension.
+        edge_project (nn.Linear): Linear layer for projecting the edge features to the same dimension as the input features.
+        activation (nn.GELU): Activation function for the adapter layers.
+
+    Methods:
+        forward(Fpet: torch.Tensor, Fect: torch.Tensor) -> torch.Tensor:
+            Performs the forward pass of the adapter module.
+
+            Args:
+                Fpet (torch.Tensor): Input features from the image encoder.
+                Fect (torch.Tensor): Edge features to be integrated.
+
+            Returns:
+                torch.Tensor: Output features after integrating the edge features.
+    """
+    def __init__(self, embed_dim: int, adapter_dim: int, scale_factor: int):
+        super(Adapter, self).__init__()
+        self.MLPktune = nn.Linear(embed_dim // scale_factor, adapter_dim)
+        self.MLPup = nn.Linear(adapter_dim, embed_dim)
+        self.edge_project = nn.Linear(1, embed_dim)  # Assuming edge features have a single channel
+        self.activation = nn.GELU()
+
+    def forward(self, F_merge: torch.Tensor) -> torch.Tensor:
+        Fk = self.MLPup(self.activation(self.MLPktune(F_merge)))
+        return Fk
diff --git a/segment_anything/modeling/sam.py b/segment_anything/modeling/sam.py
index 8074cff..b9b4e3f 100644
--- a/segment_anything/modeling/sam.py
+++ b/segment_anything/modeling/sam.py
@@ -50,7 +50,7 @@ class Sam(nn.Module):
     def device(self) -> Any:
         return self.pixel_mean.device
 
-    @torch.no_grad()
+    #@torch.no_grad() comment this out for training
     def forward(
         self,
         batched_input: List[Dict[str, Any]],
